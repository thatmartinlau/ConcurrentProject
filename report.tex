\documentclass{article}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{graphicx} % Required for inserting images

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{float}
\usepackage{amsmath}
\usepackage[parfill]{parskip}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tcolorbox}
\usepackage{dirtree}
\usepackage{algpseudocode}
\usepackage{algorithm}


\title{CSE305 Concurrent Programming: N-Body simulation project}
\author{Martin Lau, Oscar Peyron, Ziyue Qiu}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document outlines the N-Body simulation project for CSE305, which includes how we approached the problem, our project structure, code breakdown and strategies, and encountered difficulties.

To get started quickly, try typing \texttt{make nbody} in the project directory, and generate a basic gif file.
For information on how to execute the code, check our README file.

For this project, our aim is to be able to simulate systems of bodies with forces interacting with one another in 2D (such as orbiting planets around the solar system with gravitational forces, or with particles interacting with each other with Coulomb forces). This therefore includes two sections:
\begin{enumerate}
    \item Simulation of the N bodies and their evolving states.
    \item Visualization of recorded telemetries into an animated output.
\end{enumerate}

Below is a general view of our project repository file structure. 

\begin{tcolorbox}[title=Project directory summary]
    \dirtree{%
        .1 /.
        .2 output \dotfill Output gif and csv files.
            .3 telemetry.csv.
            .3 out.gif.
        .2 src \dotfill Core files.
            .3 core.cpp/hpp \dotfill Used throughout the entire proj.
            .3 simplesimulation.cpp/hpp.
            .3 barneshutt.cpp/hpp.
            .3 particlemesh.cpp/hpp.
            .3 particlemeshcuda.cpp/hpp.
            .3 particlemeshthread.cpp/hpp
        .2 py \dotfill Python testing code.
            .3 verifier.py.
        .2 makefile \dotfill Project builder. 
        .2 main.cpp \dotfill Main execution file.
        .2 mainparticlemesh.cpp \dotfill Aux particle mesh run file.
        .2 test\_telemetry.cpp \dotfill Telemetry test file.
        .2 report (PDF, TeX).
    }

\end{tcolorbox}

The work has been split as follows. The arrangement is not defining, as we like to help each other out in different parts. Our files have still been mostly separated to properly separate who worked on what functionalities.
\begin{enumerate}
    \item Martin handles general project structure, core classes, visualization, and the naive and its optimized algorithm implementation.
    \item Ziyue handles the Barnes Hutt algorithm.
    \item Oscar handles the Particle Mesh algorithm implementation (simple, thread based and cuda implementation)
\end{enumerate}


\section{Core components}

\subsection{Main elements}

There are 3 primary classes defined used throughout our project: \texttt{Vector, Body, System}. These are defined in \texttt{core.cpp/hpp.}
\begin{enumerate}
    \item \texttt{Vector} holds information on a pair of numbers, and also allows for operations with other vectors/scalars (as opposed to using \texttt{std::pair}).
    \item \texttt{Body} holds information on a given particle or body, including its mass, coordinates, velocity, and acceleration. It also contains an update method which updates its position and velocity based on acceleration. 
    \item \texttt{System} stores a collection of bodies and its recorded telemetry from the simulations we are going to do. It also contains the visualization function, which takes its recorded telemetry and outputs an animated file.
\end{enumerate} 
\textit{Note: This organization is heavily inspired from one assignment from CSE306 Computer Graphics.}

Elaborating more on the telemetry stored within the \texttt{System} class, this is stored as a vector of vector of \texttt{Vector}. What a mouthful! Our simulations creates different steps/frames. Each step/frame contains the positions of all bodies inside the system (this is a vector of \texttt{Vector}). To have the entire telemetry, we have a vector of frames, or the aforementioned data structure for the telemetry.

\subsection{Visualization code}

To visualize the code, we opt to create a gif animation after the telemetry is recorded, using the \texttt{ImageMagick/Magick++} libraries. The bulk of the visualization code is located in \texttt{core.cpp} as a method for the \texttt{System} class. As of present, the visualization function works in two steps. First, it creates the frames for the animation, then writes the frames to a gif file. The frame creation is relatively quick, and most of the visualization time is actually spent in one line (\texttt{writeImages(frames.begin(), frames.end(), name)}). 

We will look into further solutions to try and decrease the time taken to create the visualization gifs, like reducing image quality, or the number of frames. 

For testing purposes, there is also a visualizer in \texttt{visualizer.py} which creates a animation using Python's \texttt{matplotlib} library, working significantly faster, allowing us to test the correctness of our telemetries.

\begin{center}
    \includegraphics[scale=0.5]{teX/screenshot.jpg}
\end{center}

\section{Naive algorithm}

So far, we have a basic implementation of the naive algorithm without multi threading (most of my time was taken bugfixing core functionalities and getting visualization to work). 

\begin{algorithm}
    \caption{Naive simulation outline}\label{alg:cap}
    \begin{algorithmic}
        \Require System of bodies with masses $m_i$, initial positions $\vec{r}_i$, velocities $\vec{v}_i$
        \Require Time step $\Delta t$, number of steps $N$
        \State telemetry $\gets \emptyset$
        \State telemetry.append(initial positions)
        \For{step $\gets 0$ to $N-1$}
            \For{each body $i$}
                \State $\vec{a}_i \gets \vec{0}$ \Comment{Reset accelerations}
            \EndFor
            \For{$i \gets 0$ to $n-1$}
                \For{$j \gets i+1$ to $n-1$}
                    \State $\vec{F}_{ij} \gets$ ComputeGravitationalForce($body_i$, $body_j$)
                    \State $\vec{a}_i \gets \vec{a}_i + \vec{F}_{ij}/m_i$
                    \State $\vec{a}_j \gets \vec{a}_j - \vec{F}_{ij}/m_j$
                \EndFor
            \EndFor
            \For{each body $i$}
                \State $\vec{v}_i \gets \vec{v}_i + \vec{a}_i\Delta t$ \Comment{Update velocity}
                \State $\vec{r}_i \gets \vec{r}_i + \vec{v}_i\Delta t$ \Comment{Update position}
            \EndFor
            \State telemetry.append(current positions)
        \EndFor
        \Ensure Position history for all bodies stored in telemetry
    \end{algorithmic}
\end{algorithm}

Other aspects (parallelizing the update steps, parallelizing forces computations, avoiding race conditions) will be implemented later on.

\section{Particle Mesh based algorithm}
Currently I have implemented all three implementation of simple particlemesh method, thread based and cuda. I could only test the simple version as I had problems to run CUDA on my MacOS operating system. Particlemesh has currently some problems for simulating the solar system but it worked on simple example of 3-body simulation. 
\begin{algorithm}[H]
\caption{Particle Mesh Simulation (Sequential)}\label{alg:partmesh}
\begin{algorithmic}[1]
\Require System of bodies with masses $m_i$, positions $\vec{r}_i$, velocities $\vec{v}_i$
\Require Time step $\Delta t$, number of steps $N$, grid size $G$
\State telemetry $\gets \emptyset$
\State telemetry.append(initial positions)
\State Compute bounding box: $(\min \vec{r}, \max \vec{r})$
\State Expand bounds with padding proportional to domain size
\State Compute cell size: $h \gets \frac{\max_x - \min_x}{G}$
\State Initialize mass grid $M[G][G] \gets 0$
\State Initialize potential grid $\Phi[G][G] \gets 0$
\State Compute initial half-step velocities: $\vec{v}_i^{n+1/2} = \vec{v}_i + \frac{\Delta t}{2} \vec{a}_i$
\State Prepare FFTW input/output buffers and plans
\For{each step $n = 1$ to $N$}
    \State Clear mass grid $M$
    
    \For{each body $i$ in system}
        \State Determine grid cell $(x, y)$ from $\vec{r}_i$
        \State Accumulate mass: $M[x][y] \mathrel{+}= m_i$
    \EndFor

    \State Perform forward FFT on $M$ to get $\tilde{M}$
    
    \For{each frequency component $(k_x, k_y)$}
        \State Compute $k^2 = k_x^2 + k_y^2$
        \If{$k^2 > 0$}
            \State $\tilde{\Phi}[k_x][k_y] \gets -G \cdot \tilde{M}[k_x][k_y] / k^2$
        \Else
            \State $\tilde{\Phi}[k_x][k_y] \gets 0$
        \EndIf
    \EndFor

    \State Perform inverse FFT on $\tilde{\Phi}$ to get $\Phi$

    \For{each body $i$}
        \State Determine grid cell $(x, y)$ from $\vec{r}_i$
        \If{cell is interior}
            \State Compute acceleration using central finite difference on $\Phi$:
            \State $\vec{a}_i = -\nabla \Phi(x, y)$
        \Else
            \State $\vec{a}_i \gets (0, 0)$
        \EndIf
    \EndFor

    \For{each body $i$}
        \State Update half-step velocity: $\vec{v}_i^{n+1/2} \mathrel{+}= \frac{\Delta t}{2} \vec{a}_i$
        \State Set current velocity: $\vec{v}_i \gets \vec{v}_i^{n+1/2}$
        \State Update position: $\vec{r}_i \mathrel{+}= \vec{v}_i^{n+1/2} \cdot \Delta t$
    \EndFor

    \State Save current positions to telemetry
\EndFor

\State Cleanup FFTW resources
\end{algorithmic}
\end{algorithm}
\section{Barnes–Hut Algorithm}

The Barnes–Hut algorithm reduces the complexity of the classical \(O(N^2)\) N-body force computation to approximately \(O(N\log N)\) by hierarchically clustering distant bodies.  Our implementation follows these main stages:

\begin{algorithm}[H]
    \caption{Barnes–Hut Simulation Outline}\label{alg:bh}
    \begin{algorithmic}[1]
        \Require System of bodies with masses $m_i$, positions $\vec{r}_i$, velocities $\vec{v}_i$
        \Require Time step $\Delta t$, number of steps $N$, opening angle $\theta$
        \State telemetry $\gets \emptyset$
        \State telemetry.append(initial positions)
        \For{step $\gets 0$ \textbf{to} $N-1$}
            \State $[\texttt{minB},\texttt{maxB}] \gets \texttt{computeBounds}(\texttt{universe})$
            \State $\texttt{root} \gets \texttt{createRootNode}([\texttt{minB},\texttt{maxB}])$
            \For{each body $b_i$}
                \State $\texttt{insertBody}(\texttt{root}, b_i)$
            \EndFor
            \State $\texttt{computeMassDistribution}(\texttt{root})$
            \For{each body $b_i$}
                \State $\vec{a}_i \gets \texttt{forceOnBody}(b_i,\texttt{root},\theta)/m_i$
            \EndFor
            \State $\texttt{updateBodies}(\texttt{universe}, \Delta t)$
            \State $\texttt{freeQuadTree}(\texttt{root})$
            \State telemetry.append(current positions)
        \EndFor
        \Ensure Position history for all bodies stored in telemetry
    \end{algorithmic}
\end{algorithm}

\begin{enumerate}
  \item \textbf{Compute Simulation Bounds.}  
    We first call
    \begin{itemize}
      \item \texttt{computeBounds(const System\&)}  
        to find an axis-aligned square enclosing all bodies (with a small padding).
    \end{itemize}

  \item \textbf{Quadtree Construction.}  
    We represent space by a pointer-based quadtree of \texttt{QuadNode} objects:
    \begin{itemize}
      \item \texttt{createRootNode(const Bounds\&)} allocates the root covering the full region.  
      \item \texttt{insertBody(QuadNode *, Body\&)} recursively subdivides nodes so that each leaf contains at most one body.
    \end{itemize}

  \item \textbf{Mass–Center Distribution.}  
    A post-order traversal aggregates mass and center-of-mass at every internal node:
    \begin{itemize}
      \item \texttt{computeMassDistribution(QuadNode *)} computes \(\texttt{totalMass}\) and \(\texttt{centerOfMass}\).
    \end{itemize}

  \item \textbf{Force Computation.}  
    For each body \(b_i\), we traverse the tree and apply the opening-angle criterion \(\theta\):
    \begin{itemize}
      \item \texttt{forceOnBody(const Body\&, QuadNode\*, double theta)}  
        approximates distant clusters as single masses, or recurses into children when closer.
    \end{itemize}

  \item \textbf{Parallelization (Shared-Memory).}  
    To exploit multicore CPUs, we compute forces in parallel:
    \begin{itemize}
      \item \texttt{computeForcesParallel(System\&, QuadNode\*, double theta)}  
        spawns a pool of \texttt{std::thread}s, partitions the bodies into chunks, and each thread calls \texttt{forceOnBody} on its subset.  
      \item Accelerations are stored per-thread and then written back, avoiding fine-grained locks.
    \end{itemize}

  \item \textbf{Time Integration.}  
    Once all accelerations are known, we update velocities and positions in one pass:
    \begin{itemize}
      \item \texttt{updateBodies(System\&, double dt)} applies
      \[
        \mathbf{v} \gets \mathbf{v} + \mathbf{a}\,\Delta t,\quad
        \mathbf{x} \gets \mathbf{x} + \mathbf{v}\,\Delta t.
      \]
    \end{itemize}

  \item \textbf{Cleanup.}  
    The quadtree is deallocated to avoid memory leaks:
    \begin{itemize}
      \item \texttt{freeQuadTree(QuadNode *)} recursively \texttt{delete}s all nodes.
    \end{itemize}
\end{enumerate}

\subsection*{Concurrency Aspects}

\begin{itemize}
  \item \textbf{Multithreading in C++:} we illustrate basic shared-memory concurrency by partitioning the force computation across threads.  This uses standard \texttt{std::thread} and per-thread buffers, avoiding complex locking.
  \item \textbf{Concurrent Data Structures:} insertion into the quadtree could be parallelized by feeding bodies into a thread-safe queue; our current version remains serial for clarity but is structured to allow a concurrent \texttt{insertBody} using a mutex per node or a lock-free pointer array.
  \item \textbf{GPU / PRAM Illustration (Optional):} under \texttt{USE\_CUDA}, we provide \\ \texttt{simulateBruteForceGPU(System\&,double)} which launches an \(O(N^2)\) CUDA kernel.  Each GPU thread computes one body’s net force, demonstrating the PRAM model in practice.
  \item \textbf{Correctness \& Testing:} we compare parallel results to a single‐threaded reference on small \(N\), and use tools like ThreadSanitizer to detect data races.  Telemetry outputs (positions over time) are also verified for physical invariants (e.g.\ center-of-mass motion).
\end{itemize}



\end{document}
