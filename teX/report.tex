\documentclass{article}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{graphicx} % Required for inserting images

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{float}
\usepackage{amsmath}
\usepackage[parfill]{parskip}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tcolorbox}
\usepackage{dirtree}
\usepackage{algpseudocode}
\usepackage{algorithm}


\title{CSE305 Concurrent Programming: N-Body simulation project}
\author{Martin Lau, Oscar Peyron, Ziyue Qiu}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document outlines the N-Body simulation project for CSE305, which includes how we approached the problem, our project structure, code breakdown and strategies, and encountered difficulties.

To get started quickly, try typing \texttt{make nbody} in the project directory, and generate a basic gif file.
For information on how to execute the code, check our README file.

For this project, our aim is to be able to simulate systems of bodies with forces interacting with one another in 2D (such as orbiting planets around the solar system with gravitational forces, or with particles interacting with each other with Coulomb forces). This therefore includes two sections:
\begin{enumerate}
    \item Simulation of the N bodies and their evolving states.
    \item Visualization of recorded telemetries into an animated output.
\end{enumerate}

Below is a general view of our project repository file structure. 

\begin{tcolorbox}[title=Project directory summary]
    \dirtree{%
        .1 /.
        .2 output \dotfill Output gif and csv files.
            .3 telemetry.csv.
            .3 out.gif.
        .2 src \dotfill Core files.
            .3 core.cpp/hpp \dotfill Used throughout the entire proj.
            .3 simplesimulation.cpp/hpp.
            .3 barneshutt.cpp/hpp.
            .3 particlemesh.cpp/hpp.
            .3 particlemeshthread.cpp/hpp.
        .2 makefile \dotfill Project builder. 
        .2 main.cpp \dotfill Main execution file.
        .2 mainparticlemesh.cpp \dotfill Aux particle mesh run file.
        .2 test\_telemetry.cpp \dotfill Telemetry test file.
        .2 report (PDF, TeX).
    }
\end{tcolorbox}

The work has been split as follows. The arrangement is not defining, as we like to help each other out in different parts. Our files have still been mostly separated to properly separate who worked on what functionalities.
\begin{enumerate}
    \item Martin handles general project structure, core classes, visualization, and the naive and its optimized algorithm implementation.
    \item Ziyue handles the Barnes Hutt algorithm.
    \item Oscar handles the Particle Mesh algorithm implementation (simple, thread based and cuda implementation)
\end{enumerate}


\section{Core components}

\subsection{Main elements}

There are 3 primary classes defined used throughout our project: \texttt{Vector, Body, System}. These are defined in \texttt{core.cpp/hpp.}
\begin{enumerate}
    \item \texttt{Vector} holds information on a pair of numbers, and also allows for operations with other vectors/scalars (as opposed to using \texttt{std::pair}).
    \item \texttt{Body} holds information on a given particle or body, including its mass, coordinates, velocity, and acceleration. It also contains an update method which updates its position and velocity based on acceleration. 
    \item \texttt{System} stores a collection of bodies and its recorded telemetry from the simulations we are going to do. It also contains the visualization function, which takes its recorded telemetry and outputs an animated file.
\end{enumerate} 
\textit{Note: This organization is heavily inspired from one assignment from CSE306 Computer Graphics.}

Elaborating more on the telemetry stored within the \texttt{System} class, this is stored as a vector of vector of \texttt{Vector}. What a mouthful! Our simulations creates different steps/frames. Each step/frame contains the positions of all bodies inside the system (this is a vector of \texttt{Vector}). To have the entire telemetry, we have a vector of frames, or the aforementioned data structure for the telemetry.

\subsection{Visualization code}

To visualize the code, we opt to create a gif animation after the telemetry is recorded, using the \texttt{ImageMagick/Magick++} libraries. The bulk of the visualization code is located in \texttt{core.cpp} as a method for the \texttt{System} class. As of present, the visualization function works in two steps. First, it creates the frames for the animation, then writes the frames to a gif file. The frame creation is relatively quick, and most of the visualization time is actually spent in one line (\texttt{writeImages(frames.begin(), frames.end(), name)}). 

We will look into further solutions to try and decrease the time taken to create the visualization gifs, like reducing image quality, or the number of frames. 

For testing purposes, there is also a visualizer in \texttt{visualizer.py} which creates a animation using Python's \texttt{matplotlib} library, working significantly faster, allowing us to test the correctness of our telemetries.

\begin{center}
    \includegraphics[scale=0.5]{screenshot.jpg}
\end{center}

\section{Naive algorithm}

So far, we have a basic implementation of the naive algorithm without multi threading (most of my time was taken bugfixing core functionalities and getting visualization to work). 

\begin{algorithm}[H]
    \caption{Naive simulation outline}\label{alg:cap}
    \begin{algorithmic}
        \Require System of bodies with masses $m_i$, initial positions $\vec{r}_i$, velocities $\vec{v}_i$
        \Require Time step $\Delta t$, number of steps $N$
        \State telemetry $\gets \emptyset$
        \State telemetry.append(initial positions)
        \For{step $\gets 0$ to $N-1$}
            \For{each body $i$}
                \State $\vec{a}_i \gets \vec{0}$ \Comment{Reset accelerations}
            \EndFor
            \For{$i \gets 0$ to $n-1$}
                \For{$j \gets i+1$ to $n-1$}
                    \State $\vec{F}_{ij} \gets$ ComputeGravitationalForce($body_i$, $body_j$)
                    \State $\vec{a}_i \gets \vec{a}_i + \vec{F}_{ij}/m_i$
                    \State $\vec{a}_j \gets \vec{a}_j - \vec{F}_{ij}/m_j$
                \EndFor
            \EndFor
            \For{each body $i$}
                \State $\vec{v}_i \gets \vec{v}_i + \vec{a}_i\Delta t$ \Comment{Update velocity}
                \State $\vec{r}_i \gets \vec{r}_i + \vec{v}_i\Delta t$ \Comment{Update position}
            \EndFor
            \State telemetry.append(current positions)
        \EndFor
        \Ensure Position history for all bodies stored in telemetry
    \end{algorithmic}
\end{algorithm}

Other aspects (parallelizing the update steps, parallelizing forces computations, avoiding race conditions) will be implemented later on.

\section{Particle Mesh based algorithm}

I have implemented a sequential Particle mesh algorithm, as well as a thread based. 



Here below is the pseudo-code for the sequential implementation as well as for the thread-based implementation
\begin{algorithm}[H]
\caption{Particle-Mesh Simulation, using Nearest-Grid-Point (NGP)}
\begin{algorithmic}[1]
\Require System $universe$, time step $\Delta t$, grid size $N$, spatial extent $R$
\State telemetry $\gets \emptyset$
\State telemetry.append(initial positions)
\State boundaries $\gets [-R, R]$
\State Compute cell size $h \gets \frac{2R}{N}$
\State Initialize mass density grid $M[G][G] \gets 0$
\State Initialize potential grid $\Phi[G][G] \gets 0$
\State Initialize FFTW input/output arrays and plans

\For{each time step $s = 1$ to $S$}
    \State Clear mass density grid $M$

    \For{each body b in $universe$}
        \State $i \gets \left\lfloor \frac{x_b - \min_x}{h} \right\rfloor$
        \State $j \gets \left\lfloor \frac{y_b - \min_y}{h} \right\rfloor$        
        \If{$(i,j)$ in bounds}
            \State $M[i][j] \gets M[i][j] + \text{body.mass}$
        \EndIf
    \EndFor

    \State Copy grid mass to FFTW input array
    \State Compute FFT of mass density using forward FFT

    \For{each $(i,j)$ in frequency domain}
        \State Compute wave numbers $(k_x, k_y)$
        \State Compute $k^2 \gets k_x^2 + k_y^2$
        \If{$k^2 > 0$}
            \State Multiply by $-\frac{G}{k^2}$ in frequency domain
        \Else
            \State Set value to zero
        \EndIf
    \EndFor

    \State Compute inverse FFT to obtain gravitational potential
    \State Normalize inverse FFT result and store in potential grid $\Phi$

    \For{each body b in $universe$}
        \State $i \gets \left\lfloor \frac{x_b - \min_x}{h} \right\rfloor$
        \State $j \gets \left\lfloor \frac{y_b - \min_y}{h} \right\rfloor$
        \If{$(i,j)$ is valid and not at boundary}
           \State Compute force via central difference of potential:
            \State $\vec{a}_i \gets -\left( \frac{\Phi[i+1][j] - \Phi[i-1][j]}{2h}, \frac{\Phi[i][j+1] - \Phi[i][j-1]}{2h} \right)$

        \Else
            \State $\vec{a}_i \gets (0, 0)$
        \EndIf
    \EndFor

    \For{each body $i$}
        \State $\vec{v}_i \gets \vec{v}_i + \vec{a}_i \Delta t$ \Comment{Update velocity}
        \State $\vec{r}_i \gets \vec{r}_i + \vec{v}_i \Delta t$ \Comment{Update position}
    \EndFor
    \State telemetry.append(current positions)
\EndFor

\State Cleanup FFTW plans and memory
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Parallel Particle-Mesh Simulation using Nearest-Grid-Point (NGP)}
\begin{algorithmic}[1]
\Require System $universe$, time step $\Delta t$, grid size $N$, spatial extent $R$, number of threads $T$
\State telemetry $\gets \emptyset$
\State telemetry.append(initial positions)
\State boundaries $\gets [-R, R]$
\State Compute cell size $h \gets \frac{2R}{N}$
\State Initialize mass density grid $M[N][N] \gets 0$
\State Initialize potential grid $\Phi[N][N] \gets 0$
\State Initialize FFTW input/output arrays and plans

\For{each time step $s = 1$ to $S$}
    \State Clear mass density grid $M$

    \State \textbf{Parallel for} each thread $t = 1$ to $T$
        \State \quad Assign a chunk of bodies to thread $t$
        \For{each body $b$ assigned to thread $t$}
            \State $i \gets \left\lfloor \frac{x_b - \min_x}{h} \right\rfloor$
            \State $j \gets \left\lfloor \frac{y_b - \min_y}{h} \right\rfloor$        
            \If{$(i,j)$ in bounds}
                \State \textbf{Lock} $M[i][j]$
                \State $M[i][j] \gets M[i][j] + \text{body.mass}$
                \State \textbf{Unlock} $M[i][j]$
            \EndIf
        \EndFor
    \State \textbf{End parallel for}

    \State Copy mass grid to FFTW input array
    \State Compute FFT of mass density using forward FFT

    \For{each $(i,j)$ in frequency domain}
        \State Compute wave numbers $(k_x, k_y)$
        \State $k^2 \gets k_x^2 + k_y^2$
        \If{$k^2 > 0$}
            \State Multiply by $-\frac{G}{k^2}$ in frequency domain
        \Else
            \State Set value to zero
        \EndIf
    \EndFor

    \State Compute inverse FFT to obtain gravitational potential
    \State Normalize result and store in potential grid $\Phi$

    \State \textbf{Parallel for} each thread $t = 1$ to $T$
        \State \quad Assign a chunk of bodies to thread $t$
        \For{each body $b$ assigned to thread $t$}
            \State $i \gets \left\lfloor \frac{x_b - \min_x}{h} \right\rfloor$
            \State $j \gets \left\lfloor \frac{y_b - \min_y}{h} \right\rfloor$
            \If{$(i,j)$ valid and not at boundary}
                \State Compute force via central difference:
                \State $\vec{a}_b \gets -\left( \frac{\Phi[i+1][j] - \Phi[i-1][j]}{2h}, \frac{\Phi[i][j+1] - \Phi[i][j-1]}{2h} \right)$
            \Else
                \State $\vec{a}_b \gets (0, 0)$
            \EndIf
        \EndFor
    \State \textbf{End parallel for}

    \State \textbf{Parallel for} each thread $t= 1$ to $T$
        \State \quad Assign a chunk of bodies $t=1$ to $T$
            \For {each body $b$ assigned to thread $t$}
                \State $\vec{v}_b \gets \vec{v}_b + \vec{a}_b \Delta t$
                \State $\vec{r}_b \gets \vec{r}_b + \vec{v}_b \Delta t$
        \EndFor
    \State \textbf{End parallel for}

    \State telemetry.append(current positions)
\EndFor

\State Cleanup FFTW plans and memory
\end{algorithmic}
\end{algorithm}
\subsection{Time performance of the particle-mesh simulation (sequential and parallel}
The simulation where done on MacBook air with M2 chip containing 8 cores.
All simulations where done with grid size $= 10$, a spatial extent R of $10 000$ and time increment $dt = 0.2$. Additionally, each simulation includes asteroids with high radius for their orbit (between 5 and 3000  as radius)  and the rest being small asteroids with smaller radius for their orbit (between 0.5 and 5 as radius). The reason why I included big differences in radius is to allow between uniformity in the space domain. 

Parallel computation was done through parallelizing via the bodies. Each body's mass was assigned in the grid via multiple threads and the computation of their acceleration was also done through parallelization. 

The fast fourier transforms as well as the computation of the potential was not done in parallel since, assigning a thread would have been very inefficient if multiple bodies where assigned on the same grid. Indeed, locking the cells of each grid to avoid race conditions would have given a performance similar to a sequential one. 

However, one can still implement fast-fourier transform in parallel within the library fftw3. Unfortunately, I was not able to make it work on my machine. 

\textbf{Time performance of the sequential particle-mesh simulation}
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Number of Bodies} & \textbf{Execution Time (ms)} \\
\hline
4      & 52   \\
5      & 54   \\
50     & 109  \\
100    & 157  \\
1,000  & 941  \\
2,000  & 3,013 \\
5,000  & 7331 \\
10,000 & 14 053 \\
\hline
\end{tabular}
\caption{Execution time of the particle-mesh simulation for varying numbers of bodies ($\text{grid size} = 10$)}
\label{tab:performance}
\end{table}

\textbf{Time performance of the parallel particle-mesh simulation}
\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Number of Bodies} & \textbf{Threads = 5} & \textbf{Threads = 7} & \textbf{Threads = 10} \\
\hline
4     & 1661  & --    & --     \\
5     & 2209  & 2848  & 4058   \\
50    & 2295  & 2906  & 3263   \\
100   & 2298  & 3034  & 3165   \\
1000  & 3436  & 3800  & 5006   \\
2000  & 5075  & 5865  & 6595   \\
5000  & 7576  & 6244  & 6757   \\
10000 & 18230 & 12306 & 13881  \\
\hline
\end{tabular}
\caption{Execution time (in milliseconds) for varying numbers of bodies and thread counts ($\text{grid size} = 10$).}
\label{tab:thread_performance}
\end{table}

Looking at performance we see that until 2000 bodies, sequential time-performance is better than for the parallel implementation. However, for higher number of bodies, the parallel implementation with 7 threads displays better performance. 7 threads is particularly optimal as it is very close to the number of cores in the machine on which we simulate (8 cores), allowing optimal use of the CPU.  



\section{Barnes–Hut Algorithm}

The Barnes–Hut algorithm reduces the complexity of the classical \(O(N^2)\) N-body force computation to approximately \(O(N\log N)\) by hierarchically clustering distant bodies.  Our implementation follows these main stages:

\begin{algorithm}[H]
    \caption{Barnes–Hut Simulation Outline}\label{alg:bh}
    \begin{algorithmic}[1]
        \Require System of bodies with masses $m_i$, positions $\vec{r}_i$, velocities $\vec{v}_i$
        \Require Time step $\Delta t$, number of steps $N$, opening angle $\theta$
        \State telemetry $\gets \emptyset$
        \State telemetry.append(initial positions)
        \For{step $\gets 0$ \textbf{to} $N-1$}
            \State $[\texttt{minB},\texttt{maxB}] \gets \texttt{computeBounds}(\texttt{universe})$
            \State $\texttt{root} \gets \texttt{createRootNode}([\texttt{minB},\texttt{maxB}])$
            \For{each body $b_i$}
                \State $\texttt{insertBody}(\texttt{root}, b_i)$
            \EndFor
            \State $\texttt{computeMassDistribution}(\texttt{root})$
            \For{each body $b_i$}
                \State $\vec{a}_i \gets \texttt{forceOnBody}(b_i,\texttt{root},\theta)/m_i$
            \EndFor
            \State $\texttt{updateBodies}(\texttt{universe}, \Delta t)$
            \State $\texttt{freeQuadTree}(\texttt{root})$
            \State telemetry.append(current positions)
        \EndFor
        \Ensure Position history for all bodies stored in telemetry
    \end{algorithmic}
\end{algorithm}

\begin{enumerate}
  \item \textbf{Compute Simulation Bounds.}  
    We first call
    \begin{itemize}
      \item \texttt{computeBounds(const System\&)}  
        to find an axis-aligned square enclosing all bodies (with a small padding).
    \end{itemize}

  \item \textbf{Quadtree Construction.}  
    We represent space by a pointer-based quadtree of \texttt{QuadNode} objects:
    \begin{itemize}
      \item \texttt{createRootNode(const Bounds\&)} allocates the root covering the full region.  
      \item \texttt{insertBody(QuadNode *, Body\&)} recursively subdivides nodes so that each leaf contains at most one body.
    \end{itemize}

  \item \textbf{Mass–Center Distribution.}  
    A post-order traversal aggregates mass and center-of-mass at every internal node:
    \begin{itemize}
      \item \texttt{computeMassDistribution(QuadNode *)} computes \(\texttt{totalMass}\) and \(\texttt{centerOfMass}\).
    \end{itemize}

  \item \textbf{Force Computation.}  
    For each body \(b_i\), we traverse the tree and apply the opening-angle criterion \(\theta\):
    \begin{itemize}
      \item \texttt{forceOnBody(const Body\&, QuadNode\*, double theta)}  
        approximates distant clusters as single masses, or recurses into children when closer.
    \end{itemize}

  \item \textbf{Parallelization (Shared-Memory).}  
    To exploit multicore CPUs, we compute forces in parallel:
    \begin{itemize}
      \item \texttt{computeForcesParallel(System\&, QuadNode\*, double theta)}  
        spawns a pool of \texttt{std::thread}s, partitions the bodies into chunks, and each thread calls \texttt{forceOnBody} on its subset.  
      \item Accelerations are stored per-thread and then written back, avoiding fine-grained locks.
    \end{itemize}

  \item \textbf{Time Integration.}  
    Once all accelerations are known, we update velocities and positions in one pass:
    \begin{itemize}
      \item \texttt{updateBodies(System\&, double dt)} applies
      \[
        \mathbf{v} \gets \mathbf{v} + \mathbf{a}\,\Delta t,\quad
        \mathbf{x} \gets \mathbf{x} + \mathbf{v}\,\Delta t.
      \]
    \end{itemize}

  \item \textbf{Cleanup.}  
    The quadtree is deallocated to avoid memory leaks:
    \begin{itemize}
      \item \texttt{freeQuadTree(QuadNode *)} recursively \texttt{delete}s all nodes.
    \end{itemize}
\end{enumerate}

\subsection*{Concurrency Aspects}

\begin{itemize}
  \item \textbf{Multithreading in C++:} we illustrate basic shared-memory concurrency by partitioning the force computation across threads.  This uses standard \texttt{std::thread} and per-thread buffers, avoiding complex locking.
  \item \textbf{Concurrent Data Structures:} insertion into the quadtree could be parallelized by feeding bodies into a thread-safe queue; our current version remains serial for clarity but is structured to allow a concurrent \texttt{insertBody} using a mutex per node or a lock-free pointer array.
  \item \textbf{GPU / PRAM Illustration (Optional):} under \texttt{USE\_CUDA}, we provide \\ \texttt{simulateBruteForceGPU(System\&,double)} which launches an \(O(N^2)\) CUDA kernel.  Each GPU thread computes one body’s net force, demonstrating the PRAM model in practice.
  \item \textbf{Correctness \& Testing:} we compare parallel results to a single‐threaded reference on small \(N\), and use tools like ThreadSanitizer to detect data races.  Telemetry outputs (positions over time) are also verified for physical invariants (e.g.\ center-of-mass motion).
\end{itemize}



\end{document}